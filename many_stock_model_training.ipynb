{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入包\n",
    "import pandas as pd\n",
    "import tushare as ts\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense,Dropout,Conv2D,BatchNormalization,MaxPooling2D,Input,Flatten\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入token\n",
    "with open('./parameters.json','r') as f:\n",
    "    p=json.load(f)\n",
    "#     user = p['user']\n",
    "#     port = p['port']\n",
    "#     psw = p['password']\n",
    "#     host = p['host']\n",
    "    token=p['TU_share_pro_taken']\n",
    "#     cnnstr = \"mysql://\" + user + \":\" + psw + \"@\" + host + \":\" + str(port) + \"/stock?charset=utf8&use_unicode=1\"\n",
    "# engine_ts = create_engine(cnnstr)\n",
    "ts.set_token(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------参数------------------\n",
    "\n",
    "stocklist=list(pd.read_csv('./DataAnalysis/stock_list.csv').iloc[:,0])\n",
    "start='20060101'\n",
    "end='20201130'\n",
    "backwards = 66\n",
    "foresee = 5\n",
    "max_pe=100*1000\n",
    "max_pb=100*1000\n",
    "max_mv=100*1000*1000*1000\n",
    "split_point=20200501\n",
    "X_train=[]\n",
    "X_test=[]\n",
    "Y_train=[]\n",
    "Y_test=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YYYYMMDD</th>\n",
       "      <th>nt_yoy</th>\n",
       "      <th>gdp_yoy</th>\n",
       "      <th>m2_yoy</th>\n",
       "      <th>ppi_yoy</th>\n",
       "      <th>1y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20060101</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.1921</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20060102</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.1921</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20060103</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.1921</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20060104</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.1921</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20060105</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.1921</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YYYYMMDD  nt_yoy  gdp_yoy  m2_yoy  ppi_yoy  1y\n",
       "0  20060101  0.0189    0.124  0.1921   0.0305 NaN\n",
       "1  20060102  0.0189    0.124  0.1921   0.0305 NaN\n",
       "2  20060103  0.0189    0.124  0.1921   0.0305 NaN\n",
       "3  20060104  0.0189    0.124  0.1921   0.0305 NaN\n",
       "4  20060105  0.0189    0.124  0.1921   0.0305 NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取宏观数据\n",
    "Marco=pd.read_csv('./DataAnalysis/Macro2016_20201031.csv')\n",
    "# 处理宏观数据\n",
    "df3=Marco\n",
    "## 处理将所有值的部分除以100\n",
    "df3=df3.set_index('YYYYMMDD')\n",
    "df3=df3.apply(lambda x: x/100)\n",
    "df3.reset_index(inplace=True)\n",
    "df3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单只股票_处理单片数据\n",
    "def data_calculation(df):   # df 为数据框格式\n",
    "    dfa=df.copy()\n",
    "    dfa.reset_index(drop=True,inplace=True)   # 重新索引\n",
    "    dfa.insert(dfa.shape[1],'factor',np.nan)  #  增加复权列\n",
    "    dfa.loc[0,'factor']=1   #  设置起始复权值为1\n",
    "    for i in range(1,len(dfa)):   #  循环计算后续复权值\n",
    "        dfa.loc[i,'factor']=dfa.loc[i-1,'factor']*(1+dfa.loc[i,'price_change'])\n",
    "    # 重新计算开高低收的值\n",
    "    dfa['open2']=dfa['open']/dfa['close']*dfa['factor']\n",
    "    dfa['high2']=dfa['high']/dfa['close']*dfa['factor']\n",
    "    dfa['low2']=dfa['low']/dfa['close']*dfa['factor']\n",
    "    ts_code=dfa.loc[0,'ts_code']\n",
    "    trade_date=dfa.loc[backwards-1,'trade_date']\n",
    "    dfa.drop(columns=['ts_code','trade_date','open','high','low','close','pre_close','price_change'],inplace=True)  # 删除多余列\n",
    "    #  计算标签值\n",
    "    label=dfa.loc[backwards:backwards+foresee-1,'factor'].max()/dfa.loc[backwards-1,'factor']-1\n",
    "    # dfa.insert(dfa.shape[1],'label_value',label)\n",
    "    # 将换手率换算成0-1之间\n",
    "    dfa['turnover_rate']=dfa['turnover_rate']/100\n",
    "    # 删除尾部5行，确保X值在100\n",
    "    dfa.drop(dfa.tail(foresee).index,inplace=True)\n",
    "    #  返回数据框以及标签值\n",
    "    return dfa, ts_code,trade_date,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 涨幅分类\n",
    "def trend_classfication(label):\n",
    "    if label>0.05:\n",
    "        r=3\n",
    "    elif label>0 and label<=0.05:\n",
    "        r=2\n",
    "    elif label>-0.05 and label<=0:\n",
    "        r=1\n",
    "    else:\n",
    "        r=0\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单只股票处理\n",
    "def individual_stock(ts_code):\n",
    "  # 获取股票价格历史\n",
    "    daily = ts.pro_bar(ts_code=ts_code, start_date=start, end_date=end,\n",
    "                          asset='E', adj='qfq', freq='D')\n",
    "    daily.sort_values(by='trade_date',inplace=True)\n",
    "  # 获取股票日KPI\n",
    "    pro = ts.pro_api()\n",
    "    daily_basic=pro.daily_basic(ts_code=ts_code, start_date=start, end_date=end,\n",
    "                          fields='ts_code,trade_date,turnover_rate,pe_ttm,pb,total_mv')\n",
    "    daily_basic.sort_values(by='trade_date',inplace=True)\n",
    "    # 处理价格历史\n",
    "    df1=daily\n",
    "    df1.reset_index(drop=True,inplace=True)\n",
    "    df1['price_change']=(df1['close']-df1['pre_close'])/df1['pre_close']\n",
    "    df1.drop(columns=['vol','amount','change','pct_chg'],inplace=True)\n",
    "    # 处理日KPI\n",
    "    df2=daily_basic\n",
    "    ## 处理指标，都除以最大数字\n",
    "    df2['pe_ttm']=df2.pe_ttm.apply(lambda x: x/max_pe)\n",
    "    df2['pb']=df2.pb.apply(lambda x: x/max_pb)\n",
    "    df2['total_mv']=df2.total_mv.apply(lambda x: x/max_mv)\n",
    "    # 合并数据\n",
    "    df=pd.merge(df1,df2,how='left',on=['ts_code','trade_date'],left_index=False,right_index=False,copy=True)\n",
    "    df['trade_date']=df['trade_date'].astype('int64')\n",
    "    # 合并宏观数据\n",
    "    df=pd.merge(df,df3,how='left',left_on='trade_date',right_on='YYYYMMDD',left_index=False,right_index=False,copy=True)\n",
    "    # 处理合并后的数据\n",
    "    df.fillna(method='bfill',axis=0,inplace=True) # 纵向填充，用后面的值填充前面，用以补充最早缺失的数据/数据排序为从早到晚\n",
    "    df.fillna(method='ffill',axis=0,inplace=True) # 纵向填充，用前面的值填充后面，用以补充最近缺失的数据/数据排序为从早到晚\n",
    "    df.drop(columns=['YYYYMMDD'],inplace=True)\n",
    "  # 循环存入单片数据\n",
    "    if len(df)<(backwards+foresee)*2:\n",
    "        print(f'stock:{ts_code}数据量不够！')\n",
    "    else:\n",
    "        for i in range(len(df)-backwards-foresee):\n",
    "            ddd,ts_code,trade_date,label = data_calculation(df[i:i+backwards+foresee])\n",
    "            if trade_date<split_point:\n",
    "                X_train.append(np.array(ddd))\n",
    "                Y_train.append(trend_classfication(label))\n",
    "            else:\n",
    "                X_test.append(np.array(ddd))\n",
    "                Y_test.append(trend_classfication(label))        \n",
    "                # print(f'stock:{ts_code}处理完毕!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1只股票300720.SZ处理完毕\n",
      "第2只股票600678.SH处理完毕\n",
      "第3只股票000955.SZ处理完毕\n",
      "第4只股票002883.SZ处理完毕\n",
      "第5只股票600621.SH处理完毕\n",
      "第6只股票300357.SZ处理完毕\n",
      "第7只股票300701.SZ处理完毕\n",
      "第8只股票300617.SZ处理完毕\n",
      "第9只股票300696.SZ处理完毕\n",
      "第10只股票002928.SZ处理完毕\n",
      "第11只股票002677.SZ处理完毕\n",
      "第12只股票300286.SZ处理完毕\n",
      "第13只股票300461.SZ处理完毕\n",
      "第14只股票300606.SZ处理完毕\n",
      "第15只股票300661.SZ处理完毕\n",
      "第16只股票300595.SZ处理完毕\n",
      "第17只股票603360.SH处理完毕\n",
      "第18只股票002318.SZ处理完毕\n",
      "第19只股票603040.SH处理完毕\n",
      "第20只股票300558.SZ处理完毕\n",
      "第21只股票300726.SZ处理完毕\n",
      "第22只股票300299.SZ处理完毕\n",
      "第23只股票600695.SH处理完毕\n",
      "第24只股票603505.SH处理完毕\n",
      "第25只股票300624.SZ处理完毕\n",
      "第26只股票300699.SZ处理完毕\n",
      "第27只股票600605.SH处理完毕\n",
      "第28只股票300630.SZ处理完毕\n",
      "第29只股票002810.SZ处理完毕\n",
      "第30只股票300522.SZ处理完毕\n",
      "第31只股票300685.SZ处理完毕\n",
      "第32只股票600769.SH处理完毕\n",
      "第33只股票002016.SZ处理完毕\n",
      "第34只股票002568.SZ处理完毕\n",
      "第35只股票300729.SZ处理完毕\n",
      "第36只股票000099.SZ处理完毕\n",
      "第37只股票300481.SZ处理完毕\n",
      "第38只股票300585.SZ处理完毕\n",
      "第39只股票600779.SH处理完毕\n",
      "第40只股票002812.SZ处理完毕\n",
      "第41只股票300690.SZ处理完毕\n",
      "第42只股票600371.SH处理完毕\n",
      "第43只股票300470.SZ处理完毕\n",
      "第44只股票600167.SH处理完毕\n",
      "第45只股票300394.SZ处理完毕\n",
      "第46只股票601100.SH处理完毕\n",
      "第47只股票603568.SH处理完毕\n",
      "第48只股票002049.SZ处理完毕\n",
      "第49只股票002553.SZ处理完毕\n",
      "第50只股票300390.SZ处理完毕\n",
      "第51只股票300607.SZ处理完毕\n",
      "第52只股票002222.SZ处理完毕\n",
      "第53只股票002749.SZ处理完毕\n",
      "第54只股票603988.SH处理完毕\n",
      "第55只股票300033.SZ处理完毕\n",
      "第56只股票300668.SZ处理完毕\n",
      "第57只股票603239.SH处理完毕\n",
      "第58只股票002932.SZ处理完毕\n",
      "第59只股票300327.SZ处理完毕\n",
      "第60只股票300593.SZ处理完毕\n",
      "第61只股票300653.SZ处理完毕\n",
      "第62只股票600897.SH处理完毕\n",
      "第63只股票603005.SH处理完毕\n",
      "第64只股票000532.SZ处理完毕\n",
      "第65只股票300487.SZ处理完毕\n",
      "第66只股票300618.SZ处理完毕\n",
      "第67只股票603060.SH处理完毕\n",
      "第68只股票000661.SZ处理完毕\n",
      "第69只股票002372.SZ处理完毕\n",
      "第70只股票600507.SH处理完毕\n",
      "第71只股票603357.SH处理完毕\n",
      "第72只股票002887.SZ处理完毕\n",
      "第73只股票300192.SZ处理完毕\n",
      "第74只股票603338.SH处理完毕\n",
      "第75只股票300501.SZ处理完毕\n",
      "第76只股票300503.SZ处理完毕\n",
      "第77只股票300529.SZ处理完毕\n",
      "第78只股票300658.SZ处理完毕\n",
      "第79只股票002164.SZ处理完毕\n",
      "第80只股票002832.SZ处理完毕\n",
      "第81只股票002930.SZ处理完毕\n",
      "第82只股票002039.SZ处理完毕\n",
      "第83只股票002706.SZ处理完毕\n",
      "第84只股票603041.SH处理完毕\n",
      "第85只股票000655.SZ处理完毕\n",
      "第86只股票002709.SZ处理完毕\n",
      "第87只股票002801.SZ处理完毕\n",
      "第88只股票002802.SZ处理完毕\n",
      "第89只股票300432.SZ处理完毕\n",
      "第90只股票300447.SZ处理完毕\n",
      "第91只股票002107.SZ处理完毕\n",
      "第92只股票002903.SZ处理完毕\n",
      "第93只股票300502.SZ处理完毕\n",
      "第94只股票600106.SH处理完毕\n",
      "第95只股票600211.SH处理完毕\n",
      "第96只股票000672.SZ处理完毕\n",
      "第97只股票002871.SZ处理完毕\n",
      "第98只股票300628.SZ处理完毕\n",
      "第99只股票000633.SZ处理完毕\n",
      "第100只股票002690.SZ处理完毕\n",
      "第101只股票300285.SZ处理完毕\n",
      "第102只股票300669.SZ处理完毕\n",
      "第103只股票600455.SH处理完毕\n",
      "第104只股票601838.SH处理完毕\n",
      "第105只股票603305.SH处理完毕\n",
      "第106只股票603319.SH处理完毕\n",
      "第107只股票002287.SZ处理完毕\n",
      "第108只股票300395.SZ处理完毕\n",
      "第109只股票300443.SZ处理完毕\n",
      "第110只股票603583.SH处理完毕\n",
      "第111只股票603868.SH处理完毕\n",
      "第112只股票300235.SZ处理完毕\n",
      "第113只股票300697.SZ处理完毕\n",
      "第114只股票000975.SZ处理完毕\n",
      "第115只股票002414.SZ处理完毕\n",
      "第116只股票002884.SZ处理完毕\n",
      "第117只股票600519.SH处理完毕\n",
      "第118只股票600988.SH处理完毕\n",
      "第119只股票603058.SH处理完毕\n",
      "第120只股票603160.SH处理完毕\n",
      "第121只股票603666.SH处理完毕\n",
      "第122只股票002778.SZ处理完毕\n",
      "第123只股票300101.SZ处理完毕\n",
      "第124只股票603444.SH处理完毕\n",
      "第125只股票603601.SH处理完毕\n",
      "第126只股票603985.SH处理完毕\n",
      "第127只股票002901.SZ处理完毕\n",
      "第128只股票300590.SZ处理完毕\n",
      "第129只股票300639.SZ处理完毕\n",
      "第130只股票603388.SH处理完毕\n",
      "第131只股票002507.SZ处理完毕\n",
      "第132只股票002942.SZ处理完毕\n",
      "第133只股票300559.SZ处理完毕\n",
      "第134只股票300620.SZ处理完毕\n",
      "第135只股票300622.SZ处理完毕\n",
      "第136只股票603587.SH处理完毕\n",
      "第137只股票603605.SH处理完毕\n",
      "第138只股票603797.SH处理完毕\n",
      "第139只股票002214.SZ处理完毕\n",
      "第140只股票600148.SH处理完毕\n",
      "第141只股票000416.SZ处理完毕\n",
      "第142只股票000828.SZ处理完毕\n",
      "第143只股票002320.SZ处理完毕\n",
      "第144只股票002353.SZ处理完毕\n",
      "第145只股票002909.SZ处理完毕\n",
      "第146只股票002927.SZ处理完毕\n",
      "第147只股票300580.SZ处理完毕\n",
      "第148只股票600563.SH处理完毕\n",
      "第149只股票600848.SH处理完毕\n",
      "第150只股票600900.SH处理完毕\n",
      "第151只股票603488.SH处理完毕\n",
      "第152只股票603650.SH处理完毕\n",
      "第153只股票603808.SH处理完毕\n",
      "第154只股票002912.SZ处理完毕\n",
      "第155只股票300459.SZ处理完毕\n",
      "第156只股票000576.SZ处理完毕\n",
      "第157只股票002803.SZ处理完毕\n",
      "第158只股票300095.SZ处理完毕\n",
      "第159只股票600276.SH处理完毕\n",
      "第160只股票600663.SH处理完毕\n",
      "第161只股票601001.SH处理完毕\n",
      "第162只股票603136.SH处理完毕\n",
      "第163只股票603258.SH处理完毕\n",
      "第164只股票000886.SZ处理完毕\n",
      "第165只股票002829.SZ处理完毕\n",
      "第166只股票002838.SZ处理完毕\n",
      "第167只股票300012.SZ处理完毕\n",
      "第168只股票600895.SH处理完毕\n",
      "第169只股票603288.SH处理完毕\n",
      "第170只股票603848.SH处理完毕\n",
      "第171只股票002607.SZ处理完毕\n",
      "第172只股票002714.SZ处理完毕\n",
      "第173只股票300092.SZ处理完毕\n",
      "第174只股票300403.SZ处理完毕\n",
      "第175只股票300427.SZ处理完毕\n",
      "第176只股票300474.SZ处理完毕\n",
      "第177只股票300515.SZ处理完毕\n",
      "第178只股票300718.SZ处理完毕\n",
      "第179只股票600163.SH处理完毕\n",
      "第180只股票603458.SH处理完毕\n",
      "第181只股票000036.SZ处理完毕\n",
      "第182只股票000691.SZ处理完毕\n",
      "第183只股票002437.SZ处理完毕\n",
      "第184只股票300482.SZ处理完毕\n",
      "第185只股票300601.SZ处理完毕\n",
      "第186只股票600456.SH处理完毕\n",
      "第187只股票603066.SH处理完毕\n",
      "第188只股票603713.SH处理完毕\n",
      "第189只股票603878.SH处理完毕\n",
      "第190只股票000568.SZ处理完毕\n",
      "第191只股票002847.SZ处理完毕\n",
      "第192只股票300441.SZ处理完毕\n",
      "第193只股票300623.SZ处理完毕\n",
      "第194只股票300695.SZ处理完毕\n",
      "第195只股票603326.SH处理完毕\n",
      "第196只股票603809.SH处理完毕\n",
      "第197只股票002030.SZ处理完毕\n",
      "第198只股票002511.SZ处理完毕\n",
      "第199只股票002606.SZ处理完毕\n",
      "第200只股票300371.SZ处理完毕\n",
      "第201只股票603039.SH处理完毕\n",
      "第202只股票603238.SH处理完毕\n",
      "第203只股票000603.SZ处理完毕\n",
      "第204只股票000799.SZ处理完毕\n",
      "第205只股票300417.SZ处理完毕\n",
      "第206只股票603096.SH处理完毕\n",
      "第207只股票603607.SH处理完毕\n",
      "第208只股票000688.SZ处理完毕\n",
      "第209只股票000831.SZ处理完毕\n",
      "第210只股票002791.SZ处理完毕\n",
      "第211只股票002818.SZ处理完毕\n",
      "第212只股票002859.SZ处理完毕\n",
      "第213只股票600570.SH处理完毕\n",
      "第214只股票603203.SH处理完毕\n",
      "第215只股票603566.SH处理完毕\n",
      "第216只股票603580.SH处理完毕\n",
      "第217只股票603757.SH处理完毕\n",
      "第218只股票002027.SZ处理完毕\n",
      "第219只股票002117.SZ处理完毕\n",
      "第220只股票002452.SZ处理完毕\n",
      "第221只股票002555.SZ处理完毕\n",
      "第222只股票002755.SZ处理完毕\n",
      "第223只股票300415.SZ处理完毕\n",
      "第224只股票300554.SZ处理完毕\n",
      "第225只股票300722.SZ处理完毕\n",
      "第226只股票603016.SH处理完毕\n",
      "第227只股票603208.SH处理完毕\n",
      "第228只股票300019.SZ处理完毕\n",
      "第229只股票603500.SH处理完毕\n",
      "第230只股票603712.SH处理完毕\n",
      "第231只股票000935.SZ处理完毕\n",
      "第232只股票002508.SZ处理完毕\n",
      "第233只股票002605.SZ处理完毕\n",
      "第234只股票002626.SZ处理完毕\n",
      "第235只股票002651.SZ处理完毕\n",
      "第236只股票300059.SZ处理完毕\n",
      "第237只股票300408.SZ处理完毕\n",
      "第238只股票300462.SZ处理完毕\n",
      "第239只股票603596.SH处理完毕\n",
      "第240只股票603638.SH处理完毕\n",
      "第241只股票603669.SH处理完毕\n",
      "第242只股票300009.SZ处理完毕\n",
      "第243只股票300316.SZ处理完毕\n",
      "第244只股票300627.SZ处理完毕\n",
      "第245只股票300707.SZ处理完毕\n",
      "第246只股票603335.SH处理完毕\n",
      "第247只股票603811.SH处理完毕\n",
      "第248只股票002595.SZ处理完毕\n",
      "第249只股票002773.SZ处理完毕\n",
      "第250只股票300507.SZ处理完毕\n",
      "第251只股票603158.SH处理完毕\n",
      "第252只股票603277.SH处理完毕\n",
      "第253只股票000923.SZ处理完毕\n",
      "第254只股票002098.SZ处理完毕\n",
      "第255只股票300142.SZ处理完毕\n",
      "第256只股票300666.SZ处理完毕\n",
      "第257只股票600598.SH处理完毕\n",
      "第258只股票000029.SZ处理完毕\n",
      "第259只股票002007.SZ处理完毕\n",
      "第260只股票300480.SZ处理完毕\n",
      "第261只股票300600.SZ处理完毕\n",
      "第262只股票600867.SH处理完毕\n",
      "第263只股票603105.SH处理完毕\n",
      "第264只股票603533.SH处理完毕\n",
      "第265只股票603617.SH处理完毕\n",
      "第266只股票603655.SH处理完毕\n",
      "第267只股票603658.SH处理完毕\n",
      "第268只股票603886.SH处理完毕\n",
      "第269只股票002869.SZ处理完毕\n",
      "第270只股票300127.SZ处理完毕\n",
      "第271只股票300552.SZ处理完毕\n",
      "第272只股票300715.SZ处理完毕\n",
      "第273只股票300723.SZ处理完毕\n",
      "第274只股票300731.SZ处理完毕\n",
      "第275只股票601799.SH处理完毕\n",
      "第276只股票603383.SH处理完毕\n",
      "第277只股票000779.SZ处理完毕\n",
      "第278只股票300488.SZ处理完毕\n",
      "第279只股票300638.SZ处理完毕\n",
      "第280只股票002879.SZ处理完毕\n",
      "第281只股票300384.SZ处理完毕\n",
      "第282只股票300533.SZ处理完毕\n",
      "第283只股票300548.SZ处理完毕\n",
      "第284只股票603018.SH处理完毕\n",
      "第285只股票603023.SH处理完毕\n",
      "第286只股票603027.SH处理完毕\n",
      "第287只股票603037.SH处理完毕\n",
      "第288只股票603393.SH处理完毕\n",
      "第289只股票603416.SH处理完毕\n",
      "第290只股票000582.SZ处理完毕\n",
      "第291只股票000708.SZ处理完毕\n",
      "第292只股票002492.SZ处理完毕\n",
      "第293只股票002777.SZ处理完毕\n",
      "第294只股票002896.SZ处理完毕\n",
      "第295只股票300483.SZ处理完毕\n",
      "第296只股票600735.SH处理完毕\n",
      "第297只股票601330.SH处理完毕\n",
      "第298只股票603359.SH处理完毕\n",
      "第299只股票603826.SH处理完毕\n",
      "第300只股票000718.SZ处理完毕\n",
      "第301只股票002158.SZ处理完毕\n",
      "第302只股票300144.SZ处理完毕\n",
      "第303只股票600007.SH处理完毕\n",
      "第304只股票600503.SH处理完毕\n",
      "第305只股票600809.SH处理完毕\n",
      "第306只股票600846.SH处理完毕\n",
      "第307只股票603110.SH处理完毕\n",
      "第308只股票603183.SH处理完毕\n",
      "第309只股票002624.SZ处理完毕\n",
      "第310只股票002763.SZ处理完毕\n",
      "第311只股票002940.SZ处理完毕\n",
      "第312只股票300151.SZ处理完毕\n",
      "第313只股票300211.SZ处理完毕\n",
      "第314只股票300732.SZ处理完毕\n",
      "第315只股票600095.SH处理完毕\n",
      "第316只股票603578.SH处理完毕\n",
      "第317只股票603707.SH处理完毕\n",
      "第318只股票603900.SH处理完毕\n",
      "第319只股票300146.SZ处理完毕\n",
      "第320只股票300484.SZ处理完毕\n",
      "第321只股票601990.SH处理完毕\n",
      "第322只股票601997.SH处理完毕\n",
      "第323只股票000048.SZ处理完毕\n",
      "第324只股票002837.SZ处理完毕\n",
      "第325只股票002917.SZ处理完毕\n",
      "第326只股票300008.SZ处理完毕\n",
      "第327只股票300295.SZ处理完毕\n",
      "第328只股票300397.SZ处理完毕\n",
      "第329只股票300467.SZ处理完毕\n",
      "第330只股票300629.SZ处理完毕\n",
      "第331只股票601066.SH处理完毕\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for stock in stocklist:\n",
    "    try:\n",
    "        individual_stock(stock)\n",
    "    except:\n",
    "        print(\"股票处理失败：\",stock)\n",
    "        time.sleep(5)\n",
    "    print(f'第{i}只股票{stock}处理完毕')\n",
    "    i=i+1\n",
    "    # if i>3:\n",
    "    #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(X_train)\n",
    "X_test=np.array(X_test)\n",
    "Y_train=np.array(Y_train)\n",
    "Y_test=np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.reshape(X_train,X_train.shape+(1,))\n",
    "X_test=np.reshape(X_test,X_test.shape+(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(Y_train, 4)\n",
    "Y_test=to_categorical(Y_test,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((492955, 66, 13, 1), (492955, 4), (44527, 66, 13, 1), (44527, 4))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,Y_train.shape,X_test.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搭建模型\n",
    "model=Sequential()\n",
    "model.add(Conv2D(64,(9,3),input_shape=(backwards,13,1),activation='relu',data_format='channels_last'))\n",
    "model.add(MaxPooling2D(pool_size=(8,2)))\n",
    "model.add(Conv2D(64,(6,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Conv2D(32,(2,2),activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Dense(256))\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(4,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_callbacks = [\n",
    "#     callbacks.EarlyStopping(patience=2),\n",
    "#     callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "#     callbacks.TensorBoard(log_dir='./logs'),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])  # binary_crossentropy # categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 58, 11, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 7, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 2, 3, 64)          73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 222,052\n",
      "Trainable params: 222,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "7703/7703 [==============================] - 677s 88ms/step - loss: 1.0993 - accuracy: 0.4958 - val_loss: 1.1202 - val_accuracy: 0.4529\n",
      "Epoch 2/200\n",
      "7703/7703 [==============================] - 659s 86ms/step - loss: 1.0876 - accuracy: 0.5020 - val_loss: 1.1166 - val_accuracy: 0.4530\n",
      "Epoch 3/200\n",
      "7703/7703 [==============================] - 636s 83ms/step - loss: 1.0813 - accuracy: 0.5058 - val_loss: 1.1242 - val_accuracy: 0.4510\n",
      "Epoch 4/200\n",
      "7703/7703 [==============================] - 622s 81ms/step - loss: 1.0764 - accuracy: 0.5080 - val_loss: 1.1269 - val_accuracy: 0.4530\n",
      "Epoch 5/200\n",
      "7703/7703 [==============================] - 649s 84ms/step - loss: 1.0726 - accuracy: 0.5085 - val_loss: 1.1341 - val_accuracy: 0.4522\n",
      "Epoch 6/200\n",
      "7703/7703 [==============================] - 629s 82ms/step - loss: 1.0696 - accuracy: 0.5099 - val_loss: 1.1527 - val_accuracy: 0.4401\n",
      "Epoch 7/200\n",
      "7703/7703 [==============================] - 664s 86ms/step - loss: 1.0672 - accuracy: 0.5105 - val_loss: 1.1504 - val_accuracy: 0.4398\n",
      "Epoch 8/200\n",
      "7703/7703 [==============================] - 616s 80ms/step - loss: 1.0647 - accuracy: 0.5108 - val_loss: 1.1740 - val_accuracy: 0.4286\n",
      "Epoch 9/200\n",
      "7703/7703 [==============================] - 616s 80ms/step - loss: 1.0624 - accuracy: 0.5112 - val_loss: 1.1824 - val_accuracy: 0.4440\n",
      "Epoch 10/200\n",
      "7703/7703 [==============================] - 617s 80ms/step - loss: 1.0604 - accuracy: 0.5126 - val_loss: 1.1712 - val_accuracy: 0.4414\n",
      "Epoch 11/200\n",
      "7703/7703 [==============================] - 618s 80ms/step - loss: 1.0590 - accuracy: 0.5127 - val_loss: 1.1614 - val_accuracy: 0.4129\n",
      "Epoch 12/200\n",
      "7703/7703 [==============================] - 622s 81ms/step - loss: 1.0571 - accuracy: 0.5136 - val_loss: 1.1611 - val_accuracy: 0.4421\n",
      "Epoch 13/200\n",
      "7703/7703 [==============================] - 669s 87ms/step - loss: 1.0557 - accuracy: 0.5134 - val_loss: 1.1848 - val_accuracy: 0.4175\n",
      "Epoch 14/200\n",
      "7703/7703 [==============================] - 574s 75ms/step - loss: 1.0540 - accuracy: 0.5138 - val_loss: 1.1787 - val_accuracy: 0.4191\n",
      "Epoch 15/200\n",
      "7703/7703 [==============================] - 175s 23ms/step - loss: 1.0522 - accuracy: 0.5145 - val_loss: 1.1488 - val_accuracy: 0.4510\n",
      "Epoch 16/200\n",
      "7703/7703 [==============================] - 177s 23ms/step - loss: 1.0506 - accuracy: 0.5151 - val_loss: 1.1885 - val_accuracy: 0.4283\n",
      "Epoch 17/200\n",
      "7703/7703 [==============================] - 178s 23ms/step - loss: 1.0488 - accuracy: 0.5154 - val_loss: 1.1633 - val_accuracy: 0.4254\n",
      "Epoch 18/200\n",
      "7703/7703 [==============================] - 183s 24ms/step - loss: 1.0471 - accuracy: 0.5163 - val_loss: 1.2034 - val_accuracy: 0.3976\n",
      "Epoch 19/200\n",
      "7703/7703 [==============================] - 185s 24ms/step - loss: 1.0456 - accuracy: 0.5169 - val_loss: 1.1910 - val_accuracy: 0.4100\n",
      "Epoch 20/200\n",
      "7703/7703 [==============================] - 188s 24ms/step - loss: 1.0440 - accuracy: 0.5173 - val_loss: 1.1949 - val_accuracy: 0.4134\n",
      "Epoch 21/200\n",
      "7703/7703 [==============================] - 276s 36ms/step - loss: 1.0427 - accuracy: 0.5181 - val_loss: 1.2043 - val_accuracy: 0.4147\n",
      "Epoch 22/200\n",
      "7703/7703 [==============================] - 179s 23ms/step - loss: 1.0412 - accuracy: 0.5184 - val_loss: 1.2075 - val_accuracy: 0.3804\n",
      "Epoch 23/200\n",
      "7703/7703 [==============================] - 188s 24ms/step - loss: 1.0401 - accuracy: 0.5186 - val_loss: 1.2071 - val_accuracy: 0.4037\n",
      "Epoch 24/200\n",
      "7703/7703 [==============================] - 187s 24ms/step - loss: 1.0391 - accuracy: 0.5188 - val_loss: 1.2211 - val_accuracy: 0.3893\n",
      "Epoch 25/200\n",
      "7703/7703 [==============================] - 188s 24ms/step - loss: 1.0379 - accuracy: 0.5190 - val_loss: 1.2336 - val_accuracy: 0.3782\n",
      "Epoch 26/200\n",
      "7703/7703 [==============================] - 188s 24ms/step - loss: 1.0371 - accuracy: 0.5197 - val_loss: 1.2757 - val_accuracy: 0.3693\n",
      "Epoch 27/200\n",
      "7703/7703 [==============================] - 593s 77ms/step - loss: 1.0362 - accuracy: 0.5203 - val_loss: 1.2362 - val_accuracy: 0.3774\n",
      "Epoch 28/200\n",
      "7703/7703 [==============================] - 611s 79ms/step - loss: 1.0354 - accuracy: 0.5199 - val_loss: 1.2148 - val_accuracy: 0.4030\n",
      "Epoch 29/200\n",
      "7703/7703 [==============================] - 177s 23ms/step - loss: 1.0344 - accuracy: 0.5199 - val_loss: 1.2636 - val_accuracy: 0.3681\n",
      "Epoch 30/200\n",
      "7703/7703 [==============================] - 179s 23ms/step - loss: 1.0336 - accuracy: 0.5204 - val_loss: 1.2264 - val_accuracy: 0.3992\n",
      "Epoch 31/200\n",
      "7703/7703 [==============================] - 181s 24ms/step - loss: 1.0331 - accuracy: 0.5206 - val_loss: 1.2889 - val_accuracy: 0.3569\n",
      "Epoch 32/200\n",
      "7703/7703 [==============================] - 189s 25ms/step - loss: 1.0322 - accuracy: 0.5211 - val_loss: 1.2717 - val_accuracy: 0.3734\n",
      "Epoch 33/200\n",
      "7703/7703 [==============================] - 189s 24ms/step - loss: 1.0314 - accuracy: 0.5213 - val_loss: 1.2320 - val_accuracy: 0.4143\n",
      "Epoch 34/200\n",
      "7703/7703 [==============================] - 656s 85ms/step - loss: 1.0311 - accuracy: 0.5211 - val_loss: 1.2916 - val_accuracy: 0.3627\n",
      "Epoch 35/200\n",
      "7703/7703 [==============================] - 175s 23ms/step - loss: 1.0303 - accuracy: 0.5218 - val_loss: 1.3255 - val_accuracy: 0.3666\n",
      "Epoch 36/200\n",
      "7703/7703 [==============================] - 176s 23ms/step - loss: 1.0298 - accuracy: 0.5218 - val_loss: 1.2480 - val_accuracy: 0.3849\n",
      "Epoch 37/200\n",
      "7703/7703 [==============================] - 183s 24ms/step - loss: 1.0290 - accuracy: 0.5222 - val_loss: 1.2272 - val_accuracy: 0.3982\n",
      "Epoch 38/200\n",
      "7703/7703 [==============================] - 186s 24ms/step - loss: 1.0285 - accuracy: 0.5229 - val_loss: 1.3533 - val_accuracy: 0.3717\n",
      "Epoch 39/200\n",
      "7703/7703 [==============================] - 185s 24ms/step - loss: 1.0278 - accuracy: 0.5228 - val_loss: 1.3472 - val_accuracy: 0.3604\n",
      "Epoch 40/200\n",
      "7703/7703 [==============================] - 186s 24ms/step - loss: 1.0273 - accuracy: 0.5227 - val_loss: 1.3310 - val_accuracy: 0.3753\n",
      "Epoch 41/200\n",
      "7703/7703 [==============================] - 190s 25ms/step - loss: 1.0268 - accuracy: 0.5227 - val_loss: 1.2938 - val_accuracy: 0.3818\n",
      "Epoch 42/200\n",
      "7703/7703 [==============================] - 189s 25ms/step - loss: 1.0261 - accuracy: 0.5232 - val_loss: 1.2918 - val_accuracy: 0.3720\n",
      "Epoch 43/200\n",
      "7703/7703 [==============================] - 187s 24ms/step - loss: 1.0255 - accuracy: 0.5237 - val_loss: 1.3255 - val_accuracy: 0.3548\n",
      "Epoch 44/200\n",
      "7703/7703 [==============================] - 190s 25ms/step - loss: 1.0252 - accuracy: 0.5235 - val_loss: 1.3039 - val_accuracy: 0.3692\n",
      "Epoch 45/200\n",
      "7703/7703 [==============================] - 187s 24ms/step - loss: 1.0251 - accuracy: 0.5236 - val_loss: 1.3498 - val_accuracy: 0.3693\n",
      "Epoch 46/200\n",
      "7703/7703 [==============================] - 177s 23ms/step - loss: 1.0244 - accuracy: 0.5240 - val_loss: 1.3440 - val_accuracy: 0.3702\n",
      "Epoch 47/200\n",
      "7703/7703 [==============================] - 177s 23ms/step - loss: 1.0239 - accuracy: 0.5244 - val_loss: 1.3707 - val_accuracy: 0.4337\n",
      "Epoch 48/200\n",
      "7703/7703 [==============================] - 179s 23ms/step - loss: 1.0234 - accuracy: 0.5243 - val_loss: 1.3745 - val_accuracy: 0.3876\n",
      "Epoch 49/200\n",
      "7703/7703 [==============================] - 177s 23ms/step - loss: 1.0229 - accuracy: 0.5243 - val_loss: 1.4404 - val_accuracy: 0.3461\n",
      "Epoch 50/200\n",
      "7703/7703 [==============================] - 179s 23ms/step - loss: 1.0224 - accuracy: 0.5244 - val_loss: 1.3266 - val_accuracy: 0.3628\n",
      "Epoch 51/200\n",
      "7703/7703 [==============================] - 178s 23ms/step - loss: 1.0225 - accuracy: 0.5243 - val_loss: 1.3058 - val_accuracy: 0.3623\n",
      "Epoch 52/200\n",
      "7703/7703 [==============================] - 347s 45ms/step - loss: 1.0218 - accuracy: 0.5246 - val_loss: 1.3913 - val_accuracy: 0.3596\n",
      "Epoch 53/200\n",
      "7703/7703 [==============================] - 292s 38ms/step - loss: 1.0216 - accuracy: 0.5254 - val_loss: 1.4714 - val_accuracy: 0.3408\n",
      "Epoch 54/200\n",
      "7703/7703 [==============================] - 177s 23ms/step - loss: 1.0211 - accuracy: 0.5250 - val_loss: 1.5133 - val_accuracy: 0.3389\n",
      "Epoch 55/200\n",
      "7703/7703 [==============================] - 198s 26ms/step - loss: 1.0206 - accuracy: 0.5260 - val_loss: 1.3217 - val_accuracy: 0.3751\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7703/7703 [==============================] - 181s 23ms/step - loss: 1.0202 - accuracy: 0.5258 - val_loss: 1.3809 - val_accuracy: 0.3596\n",
      "Epoch 57/200\n",
      "7703/7703 [==============================] - 185s 24ms/step - loss: 1.0201 - accuracy: 0.5254 - val_loss: 1.3593 - val_accuracy: 0.3586\n",
      "Epoch 58/200\n",
      "7703/7703 [==============================] - 186s 24ms/step - loss: 1.0198 - accuracy: 0.5258 - val_loss: 1.3604 - val_accuracy: 0.3630\n",
      "Epoch 59/200\n",
      "7703/7703 [==============================] - 193s 25ms/step - loss: 1.0192 - accuracy: 0.5263 - val_loss: 1.4328 - val_accuracy: 0.3562\n",
      "Epoch 60/200\n",
      "7703/7703 [==============================] - 202s 26ms/step - loss: 1.0189 - accuracy: 0.5257 - val_loss: 1.3892 - val_accuracy: 0.3612\n",
      "Epoch 61/200\n",
      "7703/7703 [==============================] - 197s 26ms/step - loss: 1.0188 - accuracy: 0.5266 - val_loss: 1.4066 - val_accuracy: 0.3614\n",
      "Epoch 62/200\n",
      "7703/7703 [==============================] - 188s 24ms/step - loss: 1.0181 - accuracy: 0.5263 - val_loss: 1.4722 - val_accuracy: 0.3513\n",
      "Epoch 63/200\n",
      "7703/7703 [==============================] - 190s 25ms/step - loss: 1.0182 - accuracy: 0.5266 - val_loss: 1.3294 - val_accuracy: 0.3745\n",
      "Epoch 64/200\n",
      "7703/7703 [==============================] - 187s 24ms/step - loss: 1.0180 - accuracy: 0.5266 - val_loss: 1.4252 - val_accuracy: 0.3577\n",
      "Epoch 65/200\n",
      "7703/7703 [==============================] - 186s 24ms/step - loss: 1.0175 - accuracy: 0.5267 - val_loss: 1.4183 - val_accuracy: 0.3611\n",
      "Epoch 66/200\n",
      "7703/7703 [==============================] - 186s 24ms/step - loss: 1.0171 - accuracy: 0.5270 - val_loss: 1.4227 - val_accuracy: 0.3674\n",
      "Epoch 67/200\n",
      "7703/7703 [==============================] - 189s 24ms/step - loss: 1.0168 - accuracy: 0.5276 - val_loss: 1.4551 - val_accuracy: 0.3652\n",
      "Epoch 68/200\n",
      "7703/7703 [==============================] - 191s 25ms/step - loss: 1.0165 - accuracy: 0.5273 - val_loss: 1.4896 - val_accuracy: 0.3551\n",
      "Epoch 69/200\n",
      "7703/7703 [==============================] - 190s 25ms/step - loss: 1.0167 - accuracy: 0.5271 - val_loss: 1.4818 - val_accuracy: 0.3643\n",
      "Epoch 70/200\n",
      "7703/7703 [==============================] - 185s 24ms/step - loss: 1.0161 - accuracy: 0.5275 - val_loss: 1.4246 - val_accuracy: 0.3540\n",
      "Epoch 71/200\n",
      "7703/7703 [==============================] - 187s 24ms/step - loss: 1.0158 - accuracy: 0.5275 - val_loss: 1.4029 - val_accuracy: 0.3655\n",
      "Epoch 72/200\n",
      "7703/7703 [==============================] - 181s 23ms/step - loss: 1.0153 - accuracy: 0.5279 - val_loss: 1.4489 - val_accuracy: 0.3582\n",
      "Epoch 73/200\n",
      "7703/7703 [==============================] - 176s 23ms/step - loss: 1.0152 - accuracy: 0.5277 - val_loss: 1.4791 - val_accuracy: 0.3634\n",
      "Epoch 74/200\n",
      "7703/7703 [==============================] - 174s 23ms/step - loss: 1.0149 - accuracy: 0.5282 - val_loss: 1.5505 - val_accuracy: 0.3627\n",
      "Epoch 75/200\n",
      "7703/7703 [==============================] - 174s 23ms/step - loss: 1.0148 - accuracy: 0.5281 - val_loss: 1.4925 - val_accuracy: 0.3605\n",
      "Epoch 76/200\n",
      "7703/7703 [==============================] - 182s 24ms/step - loss: 1.0147 - accuracy: 0.5283 - val_loss: 1.5133 - val_accuracy: 0.3637\n",
      "Epoch 77/200\n",
      "7703/7703 [==============================] - 188s 24ms/step - loss: 1.0142 - accuracy: 0.5289 - val_loss: 1.4697 - val_accuracy: 0.3628\n",
      "Epoch 78/200\n",
      "7703/7703 [==============================] - 187s 24ms/step - loss: 1.0144 - accuracy: 0.5285 - val_loss: 1.4252 - val_accuracy: 0.3636\n",
      "Epoch 79/200\n",
      "7703/7703 [==============================] - 185s 24ms/step - loss: 1.0138 - accuracy: 0.5285 - val_loss: 1.6093 - val_accuracy: 0.3619\n",
      "Epoch 80/200\n",
      "7703/7703 [==============================] - 182s 24ms/step - loss: 1.0134 - accuracy: 0.5291 - val_loss: 1.5561 - val_accuracy: 0.3561\n",
      "Epoch 81/200\n",
      "7703/7703 [==============================] - 260s 34ms/step - loss: 1.0131 - accuracy: 0.5291 - val_loss: 1.4577 - val_accuracy: 0.3555\n",
      "Epoch 82/200\n",
      "7703/7703 [==============================] - 805s 105ms/step - loss: 1.0132 - accuracy: 0.5287 - val_loss: 1.5026 - val_accuracy: 0.3548\n",
      "Epoch 83/200\n",
      "7703/7703 [==============================] - 357s 46ms/step - loss: 1.0127 - accuracy: 0.5291 - val_loss: 1.4981 - val_accuracy: 0.3620\n",
      "Epoch 84/200\n",
      "7703/7703 [==============================] - 179s 23ms/step - loss: 1.0125 - accuracy: 0.5294 - val_loss: 1.5411 - val_accuracy: 0.3595\n",
      "Epoch 85/200\n",
      "7703/7703 [==============================] - 181s 24ms/step - loss: 1.0122 - accuracy: 0.5290 - val_loss: 1.6484 - val_accuracy: 0.3547\n",
      "Epoch 86/200\n",
      "7703/7703 [==============================] - 186s 24ms/step - loss: 1.0122 - accuracy: 0.5294 - val_loss: 1.6117 - val_accuracy: 0.3495\n",
      "Epoch 87/200\n",
      "7703/7703 [==============================] - 187s 24ms/step - loss: 1.0117 - accuracy: 0.5299 - val_loss: 1.6229 - val_accuracy: 0.3489\n",
      "Epoch 88/200\n",
      "7703/7703 [==============================] - 186s 24ms/step - loss: 1.0114 - accuracy: 0.5293 - val_loss: 1.5564 - val_accuracy: 0.3513\n",
      "Epoch 89/200\n",
      "7703/7703 [==============================] - 184s 24ms/step - loss: 1.0112 - accuracy: 0.5296 - val_loss: 1.6955 - val_accuracy: 0.3506\n",
      "Epoch 90/200\n",
      "7703/7703 [==============================] - 186s 24ms/step - loss: 1.0109 - accuracy: 0.5300 - val_loss: 1.6806 - val_accuracy: 0.3586\n",
      "Epoch 91/200\n",
      "7703/7703 [==============================] - 296s 38ms/step - loss: 1.0108 - accuracy: 0.5301 - val_loss: 1.9594 - val_accuracy: 0.3326- loss:\n",
      "Epoch 92/200\n",
      "7703/7703 [==============================] - 711s 92ms/step - loss: 1.0107 - accuracy: 0.5300 - val_loss: 1.4719 - val_accuracy: 0.3669\n",
      "Epoch 93/200\n",
      "7703/7703 [==============================] - 470s 61ms/step - loss: 1.0105 - accuracy: 0.5299 - val_loss: 1.5712 - val_accuracy: 0.3559\n",
      "Epoch 94/200\n",
      "7703/7703 [==============================] - 454s 59ms/step - loss: 1.0103 - accuracy: 0.5304 - val_loss: 1.7356 - val_accuracy: 0.3532\n",
      "Epoch 95/200\n",
      "7703/7703 [==============================] - 454s 59ms/step - loss: 1.0102 - accuracy: 0.5301 - val_loss: 1.5065 - val_accuracy: 0.3570\n",
      "Epoch 96/200\n",
      "7703/7703 [==============================] - 464s 60ms/step - loss: 1.0098 - accuracy: 0.5309 - val_loss: 1.6080 - val_accuracy: 0.3533\n",
      "Epoch 97/200\n",
      "7703/7703 [==============================] - 479s 62ms/step - loss: 1.0095 - accuracy: 0.5308 - val_loss: 1.7107 - val_accuracy: 0.3533\n",
      "Epoch 98/200\n",
      "7703/7703 [==============================] - 456s 59ms/step - loss: 1.0090 - accuracy: 0.5305 - val_loss: 1.6758 - val_accuracy: 0.3556\n",
      "Epoch 99/200\n",
      "7703/7703 [==============================] - 452s 59ms/step - loss: 1.0090 - accuracy: 0.5306 - val_loss: 1.7245 - val_accuracy: 0.3536\n",
      "Epoch 100/200\n",
      "7703/7703 [==============================] - 457s 59ms/step - loss: 1.0087 - accuracy: 0.5312 - val_loss: 1.6960 - val_accuracy: 0.3532\n",
      "Epoch 101/200\n",
      "7703/7703 [==============================] - 455s 59ms/step - loss: 1.0086 - accuracy: 0.5313 - val_loss: 1.6659 - val_accuracy: 0.3537\n",
      "Epoch 102/200\n",
      "7703/7703 [==============================] - 459s 60ms/step - loss: 1.0085 - accuracy: 0.5307 - val_loss: 1.5319 - val_accuracy: 0.3573\n",
      "Epoch 103/200\n",
      "7703/7703 [==============================] - 461s 60ms/step - loss: 1.0083 - accuracy: 0.5310 - val_loss: 1.7193 - val_accuracy: 0.3610\n",
      "Epoch 104/200\n",
      "7703/7703 [==============================] - 220s 29ms/step - loss: 1.0077 - accuracy: 0.5319 - val_loss: 1.6844 - val_accuracy: 0.3549\n",
      "Epoch 105/200\n",
      "7703/7703 [==============================] - 173s 23ms/step - loss: 1.0076 - accuracy: 0.5317 - val_loss: 1.6909 - val_accuracy: 0.3560\n",
      "Epoch 106/200\n",
      "7703/7703 [==============================] - 177s 23ms/step - loss: 1.0075 - accuracy: 0.5312 - val_loss: 1.5361 - val_accuracy: 0.3602\n",
      "Epoch 107/200\n",
      "7703/7703 [==============================] - 178s 23ms/step - loss: 1.0075 - accuracy: 0.5318 - val_loss: 1.6236 - val_accuracy: 0.3565\n",
      "Epoch 108/200\n",
      "7703/7703 [==============================] - 182s 24ms/step - loss: 1.0070 - accuracy: 0.5315 - val_loss: 1.6337 - val_accuracy: 0.3544\n",
      "Epoch 109/200\n",
      "7703/7703 [==============================] - 183s 24ms/step - loss: 1.0069 - accuracy: 0.5313 - val_loss: 1.6559 - val_accuracy: 0.3630\n",
      "Epoch 110/200\n",
      "7703/7703 [==============================] - 180s 23ms/step - loss: 1.0066 - accuracy: 0.5321 - val_loss: 1.7387 - val_accuracy: 0.3564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/200\n",
      "7703/7703 [==============================] - 179s 23ms/step - loss: 1.0065 - accuracy: 0.5321 - val_loss: 1.6511 - val_accuracy: 0.3563\n",
      "Epoch 112/200\n",
      "7703/7703 [==============================] - 183s 24ms/step - loss: 1.0061 - accuracy: 0.5323 - val_loss: 1.6539 - val_accuracy: 0.3591\n",
      "Epoch 113/200\n",
      "7703/7703 [==============================] - 188s 24ms/step - loss: 1.0058 - accuracy: 0.5325 - val_loss: 1.6665 - val_accuracy: 0.3594\n",
      "Epoch 114/200\n",
      "7703/7703 [==============================] - 191s 25ms/step - loss: 1.0060 - accuracy: 0.5319 - val_loss: 1.6608 - val_accuracy: 0.3533\n",
      "Epoch 115/200\n",
      "7703/7703 [==============================] - 192s 25ms/step - loss: 1.0058 - accuracy: 0.5326 - val_loss: 1.6851 - val_accuracy: 0.3474\n",
      "Epoch 116/200\n",
      "7703/7703 [==============================] - 184s 24ms/step - loss: 1.0058 - accuracy: 0.5324 - val_loss: 1.6406 - val_accuracy: 0.3520\n",
      "Epoch 117/200\n",
      "7703/7703 [==============================] - 179s 23ms/step - loss: 1.0054 - accuracy: 0.5326 - val_loss: 1.6582 - val_accuracy: 0.3550\n",
      "Epoch 118/200\n",
      "7703/7703 [==============================] - 181s 23ms/step - loss: 1.0051 - accuracy: 0.5331 - val_loss: 1.5768 - val_accuracy: 0.3582\n",
      "Epoch 119/200\n",
      "7703/7703 [==============================] - 180s 23ms/step - loss: 1.0050 - accuracy: 0.5326 - val_loss: 1.6317 - val_accuracy: 0.3514\n",
      "Epoch 120/200\n",
      "7703/7703 [==============================] - 253s 33ms/step - loss: 1.0049 - accuracy: 0.5333 - val_loss: 1.7644 - val_accuracy: 0.3608\n",
      "Epoch 121/200\n",
      "7703/7703 [==============================] - 459s 60ms/step - loss: 1.0044 - accuracy: 0.5333 - val_loss: 1.7066 - val_accuracy: 0.3528\n",
      "Epoch 122/200\n",
      "7703/7703 [==============================] - 463s 60ms/step - loss: 1.0048 - accuracy: 0.5326 - val_loss: 1.7410 - val_accuracy: 0.3556\n",
      "Epoch 123/200\n",
      "7703/7703 [==============================] - 449s 58ms/step - loss: 1.0042 - accuracy: 0.5330 - val_loss: 1.8258 - val_accuracy: 0.3517\n",
      "Epoch 124/200\n",
      "7703/7703 [==============================] - 458s 60ms/step - loss: 1.0044 - accuracy: 0.5334 - val_loss: 1.7613 - val_accuracy: 0.3554\n",
      "Epoch 125/200\n",
      "7703/7703 [==============================] - 454s 59ms/step - loss: 1.0036 - accuracy: 0.5336 - val_loss: 1.6073 - val_accuracy: 0.3596\n",
      "Epoch 126/200\n",
      "7703/7703 [==============================] - 462s 60ms/step - loss: 1.0039 - accuracy: 0.5332 - val_loss: 1.6267 - val_accuracy: 0.3600\n",
      "Epoch 127/200\n",
      "7703/7703 [==============================] - 457s 59ms/step - loss: 1.0036 - accuracy: 0.5334 - val_loss: 1.7307 - val_accuracy: 0.3524\n",
      "Epoch 128/200\n",
      "7703/7703 [==============================] - 454s 59ms/step - loss: 1.0033 - accuracy: 0.5335 - val_loss: 1.7275 - val_accuracy: 0.3437\n",
      "Epoch 129/200\n",
      "7703/7703 [==============================] - 458s 59ms/step - loss: 1.0031 - accuracy: 0.5335 - val_loss: 1.6924 - val_accuracy: 0.3523\n",
      "Epoch 130/200\n",
      "7703/7703 [==============================] - 465s 60ms/step - loss: 1.0033 - accuracy: 0.5339 - val_loss: 1.8157 - val_accuracy: 0.3524\n",
      "Epoch 131/200\n",
      "7703/7703 [==============================] - 459s 60ms/step - loss: 1.0031 - accuracy: 0.5343 - val_loss: 1.8913 - val_accuracy: 0.3483\n",
      "Epoch 132/200\n",
      "7703/7703 [==============================] - 463s 60ms/step - loss: 1.0033 - accuracy: 0.5341 - val_loss: 1.8756 - val_accuracy: 0.3461\n",
      "Epoch 133/200\n",
      "7703/7703 [==============================] - 387s 50ms/step - loss: 1.0028 - accuracy: 0.5336 - val_loss: 1.6975 - val_accuracy: 0.3534\n",
      "Epoch 134/200\n",
      "7703/7703 [==============================] - 169s 22ms/step - loss: 1.0023 - accuracy: 0.5339 - val_loss: 1.7853 - val_accuracy: 0.3506\n",
      "Epoch 135/200\n",
      "7703/7703 [==============================] - 171s 22ms/step - loss: 1.0020 - accuracy: 0.5347 - val_loss: 1.7203 - val_accuracy: 0.3524\n",
      "Epoch 136/200\n",
      "7703/7703 [==============================] - 175s 23ms/step - loss: 1.0017 - accuracy: 0.5348 - val_loss: 1.7789 - val_accuracy: 0.3488\n",
      "Epoch 137/200\n",
      "7703/7703 [==============================] - 177s 23ms/step - loss: 1.0021 - accuracy: 0.5341 - val_loss: 1.7645 - val_accuracy: 0.3469\n",
      "Epoch 138/200\n",
      "7703/7703 [==============================] - 182s 24ms/step - loss: 1.0019 - accuracy: 0.5339 - val_loss: 1.8321 - val_accuracy: 0.3470\n",
      "Epoch 139/200\n",
      "7703/7703 [==============================] - 398s 52ms/step - loss: 1.0018 - accuracy: 0.5343 - val_loss: 1.6800 - val_accuracy: 0.3491\n",
      "Epoch 140/200\n",
      "7703/7703 [==============================] - 472s 61ms/step - loss: 1.0011 - accuracy: 0.5343 - val_loss: 1.7945 - val_accuracy: 0.3578\n",
      "Epoch 141/200\n",
      "7703/7703 [==============================] - 469s 61ms/step - loss: 1.0012 - accuracy: 0.5343 - val_loss: 1.6984 - val_accuracy: 0.3498\n",
      "Epoch 142/200\n",
      "7703/7703 [==============================] - 465s 60ms/step - loss: 1.0012 - accuracy: 0.5344 - val_loss: 1.8167 - val_accuracy: 0.3458\n",
      "Epoch 143/200\n",
      "7703/7703 [==============================] - 467s 61ms/step - loss: 1.0013 - accuracy: 0.5346 - val_loss: 1.7952 - val_accuracy: 0.3481\n",
      "Epoch 144/200\n",
      "7703/7703 [==============================] - 467s 61ms/step - loss: 1.0013 - accuracy: 0.5343 - val_loss: 1.6765 - val_accuracy: 0.3491\n",
      "Epoch 145/200\n",
      "7703/7703 [==============================] - 468s 61ms/step - loss: 1.0006 - accuracy: 0.5347 - val_loss: 1.8363 - val_accuracy: 0.3569\n",
      "Epoch 146/200\n",
      "7703/7703 [==============================] - 466s 60ms/step - loss: 1.0006 - accuracy: 0.5351 - val_loss: 1.8883 - val_accuracy: 0.3456\n",
      "Epoch 147/200\n",
      "7703/7703 [==============================] - 443s 58ms/step - loss: 1.0006 - accuracy: 0.5351 - val_loss: 1.7971 - val_accuracy: 0.3420\n",
      "Epoch 148/200\n",
      "7703/7703 [==============================] - 172s 22ms/step - loss: 1.0002 - accuracy: 0.5357 - val_loss: 1.6920 - val_accuracy: 0.3465\n",
      "Epoch 149/200\n",
      "7703/7703 [==============================] - 174s 23ms/step - loss: 1.0004 - accuracy: 0.5354 - val_loss: 1.7072 - val_accuracy: 0.3524\n",
      "Epoch 150/200\n",
      "7703/7703 [==============================] - 174s 23ms/step - loss: 1.0003 - accuracy: 0.5348 - val_loss: 1.6639 - val_accuracy: 0.3533\n",
      "Epoch 151/200\n",
      "7703/7703 [==============================] - 178s 23ms/step - loss: 0.9999 - accuracy: 0.5354 - val_loss: 1.7309 - val_accuracy: 0.3476\n",
      "Epoch 152/200\n",
      "7703/7703 [==============================] - 182s 24ms/step - loss: 1.0001 - accuracy: 0.5351 - val_loss: 1.5561 - val_accuracy: 0.3696\n",
      "Epoch 153/200\n",
      "7703/7703 [==============================] - 183s 24ms/step - loss: 0.9999 - accuracy: 0.5355 - val_loss: 1.8471 - val_accuracy: 0.3435\n",
      "Epoch 154/200\n",
      "7703/7703 [==============================] - 323s 42ms/step - loss: 0.9996 - accuracy: 0.5360 - val_loss: 1.9688 - val_accuracy: 0.3483\n",
      "Epoch 155/200\n",
      "7703/7703 [==============================] - 475s 62ms/step - loss: 0.9996 - accuracy: 0.5357 - val_loss: 1.6831 - val_accuracy: 0.3428\n",
      "Epoch 156/200\n",
      "7703/7703 [==============================] - 459s 60ms/step - loss: 0.9994 - accuracy: 0.5353 - val_loss: 1.8752 - val_accuracy: 0.3473\n",
      "Epoch 157/200\n",
      "7703/7703 [==============================] - 458s 59ms/step - loss: 0.9993 - accuracy: 0.5351 - val_loss: 1.7832 - val_accuracy: 0.3393\n",
      "Epoch 158/200\n",
      "7703/7703 [==============================] - 447s 58ms/step - loss: 0.9991 - accuracy: 0.5352 - val_loss: 1.7829 - val_accuracy: 0.3450\n",
      "Epoch 159/200\n",
      "7703/7703 [==============================] - 172s 22ms/step - loss: 0.9990 - accuracy: 0.5358 - val_loss: 1.9454 - val_accuracy: 0.3412\n",
      "Epoch 160/200\n",
      "7703/7703 [==============================] - 173s 22ms/step - loss: 0.9987 - accuracy: 0.5362 - val_loss: 1.8523 - val_accuracy: 0.3532\n",
      "Epoch 161/200\n",
      "7703/7703 [==============================] - 177s 23ms/step - loss: 0.9992 - accuracy: 0.5362 - val_loss: 1.7593 - val_accuracy: 0.3494\n",
      "Epoch 162/200\n",
      "7703/7703 [==============================] - 184s 24ms/step - loss: 0.9988 - accuracy: 0.5361 - val_loss: 1.8316 - val_accuracy: 0.3438\n",
      "Epoch 163/200\n",
      "7703/7703 [==============================] - 181s 23ms/step - loss: 0.9983 - accuracy: 0.5361 - val_loss: 1.7119 - val_accuracy: 0.3440\n",
      "Epoch 164/200\n",
      "7703/7703 [==============================] - 378s 49ms/step - loss: 0.9984 - accuracy: 0.5356 - val_loss: 1.8768 - val_accuracy: 0.3407\n",
      "Epoch 165/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7703/7703 [==============================] - 481s 62ms/step - loss: 0.9985 - accuracy: 0.5358 - val_loss: 1.7959 - val_accuracy: 0.3399\n",
      "Epoch 166/200\n",
      "7703/7703 [==============================] - 477s 62ms/step - loss: 0.9983 - accuracy: 0.5364 - val_loss: 1.7119 - val_accuracy: 0.3560\n",
      "Epoch 167/200\n",
      "7703/7703 [==============================] - 486s 63ms/step - loss: 0.9982 - accuracy: 0.5360 - val_loss: 1.7775 - val_accuracy: 0.3438\n",
      "Epoch 168/200\n",
      "7703/7703 [==============================] - 475s 62ms/step - loss: 0.9981 - accuracy: 0.5362 - val_loss: 1.9262 - val_accuracy: 0.3415\n",
      "Epoch 169/200\n",
      "7703/7703 [==============================] - 464s 60ms/step - loss: 0.9979 - accuracy: 0.5362 - val_loss: 1.8792 - val_accuracy: 0.3327\n",
      "Epoch 170/200\n",
      "7703/7703 [==============================] - 464s 60ms/step - loss: 0.9979 - accuracy: 0.5358 - val_loss: 1.7836 - val_accuracy: 0.3497\n",
      "Epoch 171/200\n",
      "7703/7703 [==============================] - 499s 65ms/step - loss: 0.9977 - accuracy: 0.5366 - val_loss: 1.6679 - val_accuracy: 0.3446\n",
      "Epoch 172/200\n",
      "7703/7703 [==============================] - 467s 61ms/step - loss: 0.9977 - accuracy: 0.5361 - val_loss: 1.8172 - val_accuracy: 0.3534\n",
      "Epoch 173/200\n",
      "7703/7703 [==============================] - 467s 61ms/step - loss: 0.9978 - accuracy: 0.5365 - val_loss: 1.7256 - val_accuracy: 0.3508\n",
      "Epoch 174/200\n",
      "7703/7703 [==============================] - 478s 62ms/step - loss: 0.9974 - accuracy: 0.5367 - val_loss: 1.6807 - val_accuracy: 0.3467\n",
      "Epoch 175/200\n",
      "7703/7703 [==============================] - 464s 60ms/step - loss: 0.9976 - accuracy: 0.5367 - val_loss: 1.8938 - val_accuracy: 0.3454\n",
      "Epoch 176/200\n",
      "7703/7703 [==============================] - 473s 61ms/step - loss: 0.9971 - accuracy: 0.5368 - val_loss: 1.7882 - val_accuracy: 0.3476\n",
      "Epoch 177/200\n",
      "7703/7703 [==============================] - 466s 61ms/step - loss: 0.9970 - accuracy: 0.5367 - val_loss: 1.8246 - val_accuracy: 0.3478\n",
      "Epoch 178/200\n",
      "7703/7703 [==============================] - 476s 62ms/step - loss: 0.9970 - accuracy: 0.5368 - val_loss: 1.7583 - val_accuracy: 0.3472\n",
      "Epoch 179/200\n",
      "7703/7703 [==============================] - 462s 60ms/step - loss: 0.9971 - accuracy: 0.5367 - val_loss: 1.8106 - val_accuracy: 0.3375\n",
      "Epoch 180/200\n",
      "7703/7703 [==============================] - 463s 60ms/step - loss: 0.9968 - accuracy: 0.5367 - val_loss: 1.6485 - val_accuracy: 0.3542\n",
      "Epoch 181/200\n",
      "7703/7703 [==============================] - 465s 60ms/step - loss: 0.9970 - accuracy: 0.5362 - val_loss: 1.8592 - val_accuracy: 0.3512\n",
      "Epoch 182/200\n",
      "7703/7703 [==============================] - 290s 38ms/step - loss: 0.9966 - accuracy: 0.5370 - val_loss: 1.7595 - val_accuracy: 0.3517\n",
      "Epoch 183/200\n",
      "7703/7703 [==============================] - 173s 22ms/step - loss: 0.9967 - accuracy: 0.5369 - val_loss: 1.8903 - val_accuracy: 0.3401\n",
      "Epoch 184/200\n",
      "7703/7703 [==============================] - 174s 23ms/step - loss: 0.9966 - accuracy: 0.5365 - val_loss: 1.7357 - val_accuracy: 0.3485\n",
      "Epoch 185/200\n",
      "7703/7703 [==============================] - 180s 23ms/step - loss: 0.9964 - accuracy: 0.5368 - val_loss: 1.7828 - val_accuracy: 0.3469\n",
      "Epoch 186/200\n",
      "7703/7703 [==============================] - 181s 24ms/step - loss: 0.9962 - accuracy: 0.5376 - val_loss: 1.8573 - val_accuracy: 0.3462\n",
      "Epoch 187/200\n",
      "7703/7703 [==============================] - 248s 32ms/step - loss: 0.9959 - accuracy: 0.5376 - val_loss: 1.9397 - val_accuracy: 0.3395\n",
      "Epoch 188/200\n",
      "7703/7703 [==============================] - 461s 60ms/step - loss: 0.9960 - accuracy: 0.5371 - val_loss: 1.8673 - val_accuracy: 0.3490\n",
      "Epoch 189/200\n",
      "2780/7703 [=========>....................] - ETA: 3:57 - loss: 0.9946 - accuracy: 0.5380"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-2d52c639c9fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 训练数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# ,callbacks=my_callbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练数据\n",
    "history=model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=200,batch_size=64,verbose=1) # ,callbacks=my_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看模型收敛状况\n",
    "plt.plot(history.history['accuracy'],label='train_acc')\n",
    "plt.plot(history.history['val_accuracy'],label='test_acc')\n",
    "# plt.plot(history.history['loss'],label='train_loss')\n",
    "# plt.plot(history.history['val_loss'],label='test_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./DataAnalysis/many_stock_model_1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
